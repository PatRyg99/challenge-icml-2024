{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pointcloud-to-Graph Geodesic Lifting Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "This notebook shows how to import a dataset, with the desired lifting, and how to run a neural network using the loaded data.\n",
    "\n",
    "The notebook is divided into sections:\n",
    "\n",
    "- [Loading the dataset](#loading-the-dataset) loads the config files for the data and the desired tranformation, createsa a dataset object and visualizes it.\n",
    "- [Loading and applying the lifting](#loading-and-applying-the-lifting) defines a simple neural network to test that the lifting creates the expected incidence matrices.\n",
    "- [Create and run a simplicial nn model](#create-and-run-a-simplicial-nn-model) simply runs a forward pass of the model to check that everything is working as expected.\n",
    "\n",
    "***\n",
    "***\n",
    "\n",
    "Note that for simplicity the notebook is setup to use a simple graph. However, there is a set of available datasets that you can play with.\n",
    "\n",
    "To switch to one of the available datasets, simply change the *dataset_name* variable in [Dataset config](#dataset-config) to one of the following names:\n",
    "\n",
    "* cocitation_cora\n",
    "* cocitation_citeseer\n",
    "* cocitation_pubmed\n",
    "* MUTAG\n",
    "* NCI1\n",
    "* NCI109\n",
    "* PROTEINS_TU\n",
    "* AQSOL\n",
    "* ZINC\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# With this cell any imported module is reloaded before each cell execution\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from modules.data.load.loaders import PointCloudLoader\n",
    "from modules.data.preprocess.preprocessor import PreProcessor\n",
    "from modules.utils.utils import (\n",
    "    describe_data,\n",
    "    load_dataset_config,\n",
    "    load_model_config,\n",
    "    load_transform_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we just need to spicify the name of the available dataset that we want to load. First, the dataset config is read from the corresponding yaml file (located at `/configs/datasets/` directory), and then the data is loaded via the implemented `Loaders`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset configuration for gudhi_bunny:\n",
      "\n",
      "{'data_domain': 'pointcloud',\n",
      " 'data_type': 'gudhi',\n",
      " 'data_name': 'gudhi_bunny',\n",
      " 'data_dir': 'datasets/pointcloud/gudhi',\n",
      " 'task': 'regression',\n",
      " 'loss_type': 'mse',\n",
      " 'monitor_metric': 'mae'}\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"gudhi_bunny\"\n",
    "dataset_config = load_dataset_config(dataset_name)\n",
    "loader = PointCloudLoader(dataset_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then access to the data through the `load()`method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset only contains 1 sample:\n",
      " - Graph with 35947 vertices and 0 edges.\n",
      " - Features dimensions: [0, 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = loader.load()\n",
    "describe_data(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Applying the Lifting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will instantiate the lifting we want to apply to the data. For this example the clique lifting was chosen. For a clique of n nodes the algorithm for $m=3,...,max(n, complex\\_dim)$ will create simplicials for every possible combinations containing m nodes of the clique. $complex\\_dim$ is a parameter of the lifting. This is a deterministic lifting, based on connectivity, that does not modify the initial connectivity of the graph. The problem of extracting all the cliques in a graph is NP-hard, on in some formulaitons NP-complete (clique decision problem). The computational complexity of this algorithm is $O(n^k k^2)$[[1]](https://www.sciencedirect.com/science/article/pii/S0019995885800413), where $n$ is the number of nodes in the graph and $k$ is the highest clique dimension considered.\n",
    "\n",
    "***\n",
    "[[1]](https://www.sciencedirect.com/science/article/pii/S0019995885800413) Cook, S. A. (1985). A taxonomy of problems with fast parallel algorithms. Information and control, 64(1-3), 2-22.\n",
    "***\n",
    "\n",
    "For simplicial complexes creating a lifting involves creating a `SimplicialComplex` object from topomodelx and adding simplices to it using the method `add_simplices_from`. The `SimplicialComplex` class then takes care of creating all the needed matrices.\n",
    "\n",
    "Similarly to before, we can specify the transformation we want to apply through its type and id --the correxponding config files located at `/configs/transforms`. \n",
    "\n",
    "Note that the *tranform_config* dictionary generated below can contain a sequence of tranforms if it is needed.\n",
    "\n",
    "This can also be used to explore liftings from one topological domain to another, for example using two liftings it is possible to achieve a sequence such as: graph -> simplicial complex -> hypergraph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transform configuration for pointcloud2graph/geodesic_lifting:\n",
      "\n",
      "{'transform_type': 'lifting',\n",
      " 'transform_name': 'GeodesicLifting',\n",
      " 'feature_lifting': 'ProjectionSum',\n",
      " 'distance': 0.01}\n"
     ]
    }
   ],
   "source": [
    "# Define transformation type and id\n",
    "transform_type = \"liftings\"\n",
    "# If the transform is a topological lifting, it should include both the type of the lifting and the identifier\n",
    "transform_id = \"pointcloud2graph/geodesic_lifting\"\n",
    "\n",
    "# Read yaml file\n",
    "transform_config = {\n",
    "    \"lifting\": load_transform_config(transform_type, transform_id)\n",
    "    # other transforms (e.g. data manipulations, feature liftings) can be added here\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We than apply the transform via our `PreProcesor`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform parameters are the same, using existing data_dir: /Users/patrykrygiel/Documents/UTWENTE/Projects/challenge-icml-2024/datasets/pointcloud/gudhi/gudhi_bunny/lifting/298377612\n",
      "\n",
      "Dataset only contains 1 sample:\n",
      " - Graph with 35947 vertices and 7390867 edges.\n",
      " - Features dimensions: [0, 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lifted_dataset = PreProcessor(dataset, transform_config, loader.data_dir)\n",
    "describe_data(lifted_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Run a Simplicial NN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section a simple model is created to test that the used lifting works as intended. In this case the model uses the `up_laplacian_1` and the `down_laplacian_1` so the lifting should make sure to add them to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model configuration for simplicial SAN:\n",
      "\n",
      "{'in_channels': None,\n",
      " 'hidden_channels': 32,\n",
      " 'out_channels': None,\n",
      " 'n_layers': 2,\n",
      " 'n_filters': 2,\n",
      " 'order_harmonic': 5,\n",
      " 'epsilon_harmonic': 0.1}\n"
     ]
    }
   ],
   "source": [
    "from modules.models.simplicial.san import SANModel\n",
    "\n",
    "model_type = \"simplicial\"\n",
    "model_id = \"san\"\n",
    "model_config = load_model_config(model_type, model_id)\n",
    "\n",
    "model = SANModel(model_config, dataset_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model(lifted_dataset.get(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything is correct the cell above should execute without errors. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_topox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
